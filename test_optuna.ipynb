{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 01:13:35.630448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743722015.643240  521996 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743722015.647148  521996 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-04 01:13:35.662835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 01:13:39.843138: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "from ModelClass import RegressionModel\n",
    "from DatasetClass import Dataset, DatasetPt, DatasetMass\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from src.helpers import make_filter_slice\n",
    "import tensorflow as tf\n",
    "# import sys\n",
    "# sys.path.append(\"/home/kvake/.local/lib/python3.11/site-packages\")\n",
    "import optuna\n",
    "# sys.path.append(\"/home/kvake/.local/lib/python3.9/site-packages\")  # Adjust if needed\n",
    "import optuna_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "erik_data = \"/scratch/ucjf-atlas/htautau/SM_Htautau_R22/V02_skim_mva_01/*/*/*/*/*H125*.root\"\n",
    "patrik_data = \"/scratch/ucjf-atlas/htautau/SM_Htautau_R22/V02_skim_mva_01/*/*/*/*/*Ztt*.root\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6523684\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "# dataset.build_dataset()\n",
    "# dataset.save_data()\n",
    "dataset.load_data()\n",
    "\n",
    "print(len(dataset.train_dataset))\n",
    "\n",
    "#This block of code iterates through the dataset and extracts the pt values of the labels and stores them in a list\n",
    "# data = [labels.numpy()[0] for features, labels in dataset.train_dataset.take(100000)]\n",
    "\n",
    "# plt.hist(data, bins=100, range=(50, 130), histtype='step', label='pt distribution', density=False)\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('pt distribution of the dataset')\n",
    "# plt.xlabel('pt')\n",
    "# plt.ylabel('Number of events')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 6523684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-04 01:13:56,351] A new study created in RDB with name: Higgs_analysis_jupyter_5\n",
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://0.0.0.0:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n",
      "/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [100, 10000] and step=1000, but the range is not divisible by `step`. It will be replaced by [100, 9100].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching datasets...\n",
      "Building model...\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 01:13:56.930012: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 112/1631\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:52\u001b[0m 192ms/step - loss: 23281.1738 - mean_absolute_percentage_error: 88.8686 - mean_squared_error: 23281.1719"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 71\u001b[0m\n\u001b[1;32m     66\u001b[0m optuna_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# 5. Print Results\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# ========================\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43moptuna_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for Optuna to finish before printing results\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of finished trials:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "from DatasetClass import DatasetPt\n",
    "from ModelClass import RegressionModel\n",
    "import threading\n",
    "\n",
    "# ======================\n",
    "# 1. Build/Load Dataset (Outside Optuna Loop for Efficiency)\n",
    "# ======================\n",
    "dataset = DatasetPt()\n",
    "dataset.load_data()\n",
    "\n",
    "print(\"Train dataset size:\", len(dataset.train_dataset))\n",
    "\n",
    "# ============================\n",
    "# 2. Define Optuna Objective (Fast Training)\n",
    "# ============================\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter search.\n",
    "    \"\"\"\n",
    "    model = RegressionModel(\n",
    "        dataset,\n",
    "        n_layers=trial.suggest_int('n_layers', 3, 6),\n",
    "        hidden_layer_size=2048,\n",
    "        initial_learning_rate=trial.suggest_float('initial_learning_rate', 1e-3, 1e-2, log=True),\n",
    "        n_epochs=80,\n",
    "        activation_function  = 'relu',\n",
    "        batch_size=trial.suggest_int('batch_size', 3000, 5000, step=1000),\n",
    "        dropout_rate=0.2,\n",
    "        weight_decay= 1e-5,\n",
    "        optimizer='adamw',\n",
    "        n_normalizer_samples = trial.suggest_int('n_normalizer_samples', 100, 10000, step=1000)\n",
    "    )\n",
    "\n",
    "    model.prepare_dataset()\n",
    "    model.create_normalizer()\n",
    "    model.build_model()\n",
    "    model.train_model()\n",
    "\n",
    "    val_loss, _ = model.model.evaluate(model.val_batch, verbose=0)\n",
    "    return val_loss\n",
    "\n",
    "# ========================\n",
    "# 3. Run the Optuna Study\n",
    "# ========================\n",
    "study = optuna.create_study(storage=\"sqlite:///optuna.db\", study_name=\"Higgs_analysis_jupyter_5\", direction='minimize')\n",
    "\n",
    "# ========================\n",
    "# 4. Start `optuna-dashboard` in a Separate Thread\n",
    "# ========================\n",
    "\n",
    "def run_dashboard():\n",
    "    server = run_server(\"sqlite:///optuna.db\", host=\"0.0.0.0\", port=8080)\n",
    "    server.run()  # Run dashboard continuously in a separate thread\n",
    "\n",
    "dashboard_thread = threading.Thread(target=run_dashboard)\n",
    "dashboard_thread.daemon = True  # Ensures it stops when the script ends\n",
    "dashboard_thread.start()\n",
    "\n",
    "def run_optuna():\n",
    "    study.optimize(objective, n_trials=10, n_jobs=1)\n",
    "\n",
    "# Run Optuna in a separate thread so the dashboard can be accessed in real-time\n",
    "optuna_thread = threading.Thread(target=run_optuna)\n",
    "optuna_thread.start()\n",
    "\n",
    "# ========================\n",
    "# 5. Print Results\n",
    "# ========================\n",
    "optuna_thread.join()  # Wait for Optuna to finish before printing results\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n",
    "# Dashboard will keep running in background\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htt_env",
   "language": "python",
   "name": "htt_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
