{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 22:42:27.295456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743025347.308738 3454435 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743025347.312781 3454435 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-26 22:42:27.327634: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from ModelClass import RegressionModel\n",
    "from DatasetClass import Dataset, DatasetMass\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from src.helpers import make_filter_slice\n",
    "import tensorflow as tf\n",
    "import optuna \n",
    "from optuna_dashboard import run_server\n",
    "import threading\n",
    "#import sys\n",
    "#import logging \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "erik_data = \"/scratch/ucjf-atlas/htautau/SM_Htautau_R22/V02_skim_mva_01/*/*/*/*/*H125*.root\"\n",
    "patrik_data = \"/scratch/ucjf-atlas/htautau/SM_Htautau_R22/V02_skim_mva_01/*/*/*/*/*Ztt*.root\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7799005\n",
      "7799005\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetMass(file_paths=patrik_data)\n",
    "dataset.load_data(file_name = \"data\")\n",
    "#start_time = time.time()\n",
    "#for batch in dataset.train_dataset.take(1):\n",
    "#    print(\"Batch loaded in:\", time.time() - start_time)\n",
    "#train_batches = dataset.train_dataset.batch(32)\n",
    "#for feature, target in train_batches.take(1):\n",
    "#    print(f\"Feature: {feature.shape}\")\n",
    "#    print(f\"Target: {target.shape}\")\n",
    "#print(dataset.train_events)\n",
    "#print(len(dataset.train_dataset))\n",
    "#print(len(dataset.dev_dataset))\n",
    "#print(len(dataset.val_dataset))\n",
    "#This block of code iterates through the dataset and extracts the pt values of the labels and stores them in a list\n",
    "#data = [labels.numpy()[0] for features, labels in dataset.train_dataset.take(100000)]\n",
    "\n",
    "#plt.hist(data, bins=100, range=(50, 130), histtype='step', label='pt distribution', density=False)\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.title('pt distribution of the dataset')\n",
    "#plt.xlabel('pt')\n",
    "#plt.ylabel('Number of events')\n",
    "#plt.show()\n",
    "\n",
    "#features, masses = next(iter(dataset.train_dataset.batch(1000)))\n",
    "\n",
    "#print(features.shape)\n",
    "#print(features)\n",
    "\n",
    "#plt.hist(masses, range=(70, 130), bins=50)\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.make_slices(n_slices=100)\n",
    "slices = dataset.slices   \n",
    "\n",
    "lorentz_mask = tf.constant(dataset.get_lorentz_mask())  # Shape [35] bool values\n",
    "lorentz_indices = tf.squeeze(tf.where(lorentz_mask), axis=1) # [0 1 2 3 4 5 6 7 13 14 ...] \n",
    "n_vectors = tf.shape(lorentz_indices)[0] // 4 # number of 4-vectors\n",
    "lorentz_indices_4d = tf.reshape(lorentz_indices, (n_vectors, 4))  # [n_vectors, 4]\n",
    "\n",
    "@tf.function\n",
    "def augment_lorentz(data, target):\n",
    "    beta = tf.random.uniform(shape = (), minval=-0.98, maxval=0.98) # Shape ()\n",
    "    #tf.print(\"used beta: \", beta)\n",
    "    gamma = 1.0 / tf.sqrt(1.0 - beta**2) # Shape ()\n",
    "    #tf.print(\"used gamma: \", gamma)\n",
    "    \n",
    "    for i in range(n_vectors):\n",
    "        vec_indices = lorentz_indices_4d[i] # Shape (4,)\n",
    "        pt = data[vec_indices[0]] # scalar values\n",
    "        eta = data[vec_indices[1]]\n",
    "        #tf.print(\"eta: \", eta)\n",
    "        phi = data[vec_indices[2]]\n",
    "        E = data[vec_indices[3]]\n",
    "        #tf.print(\"E: \", E)\n",
    "        #print(E.shape)\n",
    "        \n",
    "        # Convert to Cartesian coordinates\n",
    "        #px = pt * tf.cos(phi)\n",
    "        #py = pt * tf.sin(phi)\n",
    "        pz = pt * tf.sinh(eta) # Shape ()\n",
    "        #tf.print(\"pz: \", pz)\n",
    "\n",
    "        E_prime = gamma * (E - beta * pz) # Shape ()\n",
    "        #tf.print(\"E_prime: \", E_prime)\n",
    "        pz_prime = gamma * (pz - beta * E) \n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        eta_prime = tf.asinh(pz_prime / (pt + epsilon)) # Shape ()\n",
    "        #tf.print(\"eta_prime: \", eta_prime)\n",
    "        update_indices = tf.reshape(vec_indices, [-1, 1])  # Shape (4, 1)\n",
    "        update_values = tf.stack([pt, eta_prime, phi, E_prime]) # Shape (4,)\n",
    "        # Update the tensor\n",
    "        data = tf.tensor_scatter_nd_update(\n",
    "            data,\n",
    "            indices=update_indices,\n",
    "            updates=update_values\n",
    "        )\n",
    "        #print(data.shape)\n",
    "        \n",
    "    return data, target\n",
    "\n",
    "n_events = 10000\n",
    "new_dataset = tf.data.Dataset.sample_from_datasets([s.repeat() for s in slices], weights=[1.]*len(slices))\n",
    "#orig_features, orig_masses = next(iter(new_dataset))\n",
    "#print(orig_features)\n",
    "#print(orig_masses)\n",
    "new_dataset = new_dataset.take(n_events)\n",
    "augmented_dataset = new_dataset.map(augment_lorentz)\n",
    "#new_features, new_masses = next(iter(new_dataset))\n",
    "#print(new_features)š\n",
    "#print(new_masses)\n",
    "#batch_dataset = augmented_dataset.batch(n_events)\n",
    "#print(f\"Number of events in batch_dataset: {len(augmented_dataset)}\")\n",
    "#features, masses = next(iter(batch_dataset))\n",
    "\n",
    "#plt.hist(masses, range=(70, 130), bins=50)\n",
    "#plt.show()\n",
    "\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lorentz_mask = tf.constant(dataset.get_lorentz_mask()) \n",
    "print(lorentz_mask)\n",
    "\n",
    "features = tf.constant([\n",
    "    5.56329613e+01, -3.16469967e-01, -9.03481603e-01, 6.74349565e-07,\n",
    "    5.47039528e+01, -1.49002433e-01, 2.52283901e-01, 1.05658375e-01,\n",
    "    1.67467535e-01, 1.15576553e+00, 1.16783524e+00, 1.38526611e+02,\n",
    "    1.10336914e+02, 1.38102692e+02, 1.74350947e-01, 2.87319326e+00,\n",
    "    1.55200872e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
    "    0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
    "    0.00000000e+00, 4.71827660e+01, 0.00000000e+00, -6.81111366e-02,\n",
    "    -1.58858541e-02, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "    0.00000000e+00, 1.00000000e+00, 1.00000000e+00\n",
    "], dtype=tf.float32)\n",
    "\n",
    "label = tf.constant(94.92591, dtype=tf.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensors((features, label))\n",
    "dataset = dataset.map(augment_lorentz)\n",
    "\n",
    "new_features, new_label = next(iter(dataset))\n",
    "print(new_features)\n",
    "print(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.make_slices(n_slices=10)\n",
    "slices = dataset.slices \n",
    "phi_mask = tf.constant(dataset.get_phi_mask())\n",
    "\n",
    "@tf.function\n",
    "def augment_phi(data, target):\n",
    "    # generate random rotation angle\n",
    "    angle = tf.random.uniform(shape=(tf.shape(data)[0],), minval=-np.pi, maxval=np.pi)\n",
    "    #tf.print(\"angle: \", angle.shape)\n",
    "    #tf.print(\"data: \", data.shape)\n",
    "    # apply rotation\n",
    "    data  = tf.where(phi_mask, data + angle, data)\n",
    "    \n",
    "    # normalize angles between -pi and pi\n",
    "    data = tf.where(phi_mask, tf.math.atan2(tf.sin(data), tf.cos(data)), data)\n",
    "    \n",
    "    return data, target\n",
    "\n",
    "# sample from the slices\n",
    "n_events = 1000\n",
    "new_dataset = tf.data.Dataset.sample_from_datasets([s.repeat() for s in slices], weights=[1.]*len(slices))\n",
    "new_dataset = new_dataset.take(n_events)\n",
    "\n",
    "# apply augmentation\n",
    "new_dataset = new_dataset.map(augment_phi)\n",
    "#aug_dataset = new_dataset.batch(n_events)\n",
    "\n",
    "\n",
    "#print(f\"Number of events in batch_dataset: {len(batch_dataset)}\")\n",
    "#features, masses = next(iter(batch_dataset))\n",
    "#print(features.shape)\n",
    "#print(features)\n",
    "\n",
    "#plt.hist(masses, range=(70, 130), bins=50)\n",
    "#plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'batch_size': [3200, 6400],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'epochs': [5, 20, 30]\n",
    "}\n",
    "\n",
    "iterable = list(itertools.product(*param_grid.values()))\n",
    "for params in iterable:\n",
    "    model = RegressionModel(dataset=dataset, batch_size=params[0], initial_learning_rate=params[1], n_epochs=params[2])\n",
    "    model.prepare_dataset()\n",
    "    model.create_normalizer()\n",
    "    model.build_model()\n",
    "    model.train_model()\n",
    "    model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-25 23:41:16,016] Using an existing study with name 'Higgs_analysis_mass_2' instead of creating a new one.\n",
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://0.0.0.0:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 23:41:16.510918: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742942477.678492 2786360 service.cc:148] XLA service 0x7f7c1c005ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742942477.678524 2786360 service.cc:156]   StreamExecutor device (0): NVIDIA L40S, Compute Capability 8.9\n",
      "2025-03-25 23:41:17.711543: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1742942477.828448 2786360 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   18/12185\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 6ms/step - loss: 10386.6963 - mean_absolute_percentage_error: 97.7805 - mean_squared_error: 10386.6973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742942478.697891 2786360 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12182/12185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1250.8641 - mean_absolute_percentage_error: 15.4898 - mean_squared_error: 1250.8638"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 23:42:45.404916: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67_0', 228 bytes spill stores, 228 bytes spill loads\n",
      "\n",
      "2025-03-25 23:42:45.449078: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67_0', 756 bytes spill stores, 700 bytes spill loads\n",
      "\n",
      "2025-03-25 23:42:57.371296: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-03-25 23:42:57.513674: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67_0', 324 bytes spill stores, 344 bytes spill loads\n",
      "\n",
      "2025-03-25 23:42:57.598710: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_67_0', 1000 bytes spill stores, 712 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12185/12185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 8ms/step - loss: 1250.7100 - mean_absolute_percentage_error: 15.4887 - mean_squared_error: 1250.7095 - val_loss: 680.5740 - val_mean_absolute_percentage_error: 7.1774 - val_mean_squared_error: 680.8720\n",
      "Epoch 2/20\n",
      "\u001b[1m    1/12185\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:16:50\u001b[0m 969ms/step - loss: 1592.9148 - mean_absolute_percentage_error: 101.2956 - mean_squared_error: 1592.9148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12185/12185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 924us/step - loss: 1592.9148 - mean_absolute_percentage_error: 101.2956 - mean_squared_error: 1592.9148 - val_loss: 707.6653 - val_mean_absolute_percentage_error: 7.4744 - val_mean_squared_error: 707.9465\n",
      "Epoch 3/20\n",
      "\u001b[1m12185/12185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - loss: 536.9959 - mean_absolute_percentage_error: 10.2609 - mean_squared_error: 536.9961 - val_loss: 681.8136 - val_mean_absolute_percentage_error: 7.7511 - val_mean_squared_error: 682.0729\n",
      "Epoch 4/20\n",
      "\u001b[1m12185/12185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 896us/step - loss: 1529.3429 - mean_absolute_percentage_error: 102.1909 - mean_squared_error: 1529.3429 - val_loss: 716.2120 - val_mean_absolute_percentage_error: 8.5863 - val_mean_squared_error: 716.4556\n",
      "Epoch 5/20\n",
      "\u001b[1m 8609/12185\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 475.4726 - mean_absolute_percentage_error: 9.7364 - mean_squared_error: 475.4728"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m optuna_thread \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mrun_optuna)\n\u001b[1;32m     45\u001b[0m optuna_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 47\u001b[0m \u001b[43moptuna_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for Optuna to finish before printing results\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of finished trials:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib64/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1081\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8610/12185\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 7ms/step - loss: 475.4728 - mean_absolute_percentage_error: 9.7364 - mean_squared_error: 475.4730"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 9085/12185\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 475.6042 - mean_absolute_percentage_error: 9.7359 - mean_squared_error: 475.6045"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-25 23:46:08,653] Trial 50 failed with parameters: {'n_layers': 2, 'hidden_layer_size': 768, 'initial_learning_rate': 0.0034933707772084595, 'n_epochs': 20, 'batch_size': 640, 'dropout_rate': 0.39597475121024717, 'weight_decay': 9.481872484282561e-05} because of the following error: ZMQError('Socket operation on non-socket').\n",
      "Traceback (most recent call last):\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2786236/4236431915.py\", line 19, in objective\n",
      "    model.train_model()\n",
      "  File \"/home/ivanpa/HiggsAnalysisBc/ModelClass.py\", line 138, in train_model\n",
      "    history = self.model.fit(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 694, in write\n",
      "    self._schedule_flush()\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 590, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 694, in write\n",
      "    self._schedule_flush()\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 590, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Call stack:\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 937, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_2786236/4236431915.py\", line 41, in run_optuna\n",
      "    study.optimize(objective, n_trials=100, n_jobs=1)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/study.py\", line 475, in optimize\n",
      "    _optimize(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
      "    frozen_trial = _run_trial(study, func, catch)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 226, in _run_trial\n",
      "    _log_failed_trial(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 258, in _log_failed_trial\n",
      "    _logger.warning(\n",
      "Message: \"Trial 50 failed with parameters: {'n_layers': 2, 'hidden_layer_size': 768, 'initial_learning_rate': 0.0034933707772084595, 'n_epochs': 20, 'batch_size': 640, 'dropout_rate': 0.39597475121024717, 'weight_decay': 9.481872484282561e-05} because of the following error: ZMQError('Socket operation on non-socket').\"\n",
      "Arguments: ()\n",
      "[W 2025-03-25 23:46:08,660] Trial 50 failed with value None.\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1087, in emit\n",
      "    self.flush()\n",
      "  File \"/usr/lib64/python3.9/logging/__init__.py\", line 1067, in flush\n",
      "    self.stream.flush()\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Call stack:\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 937, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_2786236/4236431915.py\", line 41, in run_optuna\n",
      "    study.optimize(objective, n_trials=100, n_jobs=1)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/study.py\", line 475, in optimize\n",
      "    _optimize(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
      "    frozen_trial = _run_trial(study, func, catch)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 226, in _run_trial\n",
      "    _log_failed_trial(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 265, in _log_failed_trial\n",
      "    _logger.warning(\"Trial {} failed with value {}.\".format(trial.number, repr(value_or_values)))\n",
      "Message: 'Trial 50 failed with value None.'\n",
      "Arguments: ()\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_2786236/4236431915.py\", line 41, in run_optuna\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/study.py\", line 475, in optimize\n",
      "    _optimize(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
      "    _optimize_sequential(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
      "    frozen_trial = _run_trial(study, func, catch)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n",
      "    raise func_err\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2786236/4236431915.py\", line 19, in objective\n",
      "  File \"/home/ivanpa/HiggsAnalysisBc/ModelClass.py\", line 138, in train_model\n",
      "    history = self.model.fit(\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 694, in write\n",
      "    self._schedule_flush()\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 590, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception in threading.excepthook:\n",
      "Exception ignored in thread started by: <bound method Thread._bootstrap of <Thread(Thread-5, stopped 140170936440384)>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 937, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 982, in _bootstrap_inner\n",
      "    self._invoke_excepthook(self)\n",
      "  File \"/usr/lib64/python3.9/threading.py\", line 1264, in invoke_excepthook\n",
      "    local_print(\"Exception in threading.excepthook:\",\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/singularity/ucjf/venv_tf_218/lib64/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    model = RegressionModel(\n",
    "                dataset,\n",
    "                n_layers             = trial.suggest_int('n_layers', 2, 5),\n",
    "                hidden_layer_size    = trial.suggest_int('hidden_layer_size', 512, 1024, step=128),\n",
    "                initial_learning_rate= trial.suggest_float('initial_learning_rate', 1e-3, 1e-2, log=True),\n",
    "                n_epochs             = trial.suggest_int('n_epochs', 10, 20, step=5),\n",
    "                activation_function  = 'relu',\n",
    "                batch_size           = trial.suggest_int('batch_size', 512, 1024,step=64),\n",
    "                dropout_rate         = trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "                weight_decay         = trial.suggest_float('weight_decay', 1e-5, 1e-4, log=True)\n",
    "            )\n",
    "\n",
    "    model.prepare_dataset()\n",
    "    model.create_normalizer()\n",
    "    model.build_model()\n",
    "    model.train_model()\n",
    "\n",
    "    # Handle based on the number of metrics\n",
    "    evaluation_results = model.model.evaluate(model.dev_batch, verbose=0)\n",
    "    if isinstance(evaluation_results, list):\n",
    "        val_loss = evaluation_results[0]\n",
    "    else:\n",
    "        val_loss = evaluation_results  # If only one value is returned, it's the loss\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(storage=\"sqlite:///optuna.db\", study_name=\"Higgs_analysis_mass_2\", direction='minimize', load_if_exists=True)\n",
    "\n",
    "def run_dashboard():\n",
    "    server = run_server(\"sqlite:///optuna.db\", host=\"0.0.0.0\", port=8080)\n",
    "    server.run()  # Run dashboard continuously in a separate thread\n",
    "\n",
    "dashboard_thread = threading.Thread(target=run_dashboard)\n",
    "dashboard_thread.daemon = True  # Ensures it stops when the script ends\n",
    "dashboard_thread.start()\n",
    "\n",
    "def run_optuna():\n",
    "    study.optimize(objective, n_trials=100, n_jobs=1)\n",
    "\n",
    "# Run Optuna in a separate thread so the dashboard can be accessed in real-time\n",
    "optuna_thread = threading.Thread(target=run_optuna)\n",
    "optuna_thread.start()\n",
    "\n",
    "optuna_thread.join()  # Wait for Optuna to finish before printing results\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htt_env",
   "language": "python",
   "name": "htt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
