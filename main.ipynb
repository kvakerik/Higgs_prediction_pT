{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelClass import RegressionModel\n",
    "from DatasetClass import Dataset, DatasetMass\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from src.helpers import make_filter_slice\n",
    "import tensorflow as tf\n",
    "import optuna \n",
    "from optuna_dashboard import run_server\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "erik_data = \"/scratch/ucjf-atlas/htautau/SM_Htautau_R22/V02_skim_mva_01/*/*/*/*/*H125*.root\"\n",
    "patrik_data = \"/scratch/ucjf-atlas/htautau/SM_Htautau_R22/V02_skim_mva_01/*/*/*/*/*Ztt*.root\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7799005\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetMass(file_paths=patrik_data, file_name = \"data\")\n",
    "dataset.load_data()\n",
    "print(dataset.train_events)\n",
    "    \n",
    "#dataset.augment_data_lorentz(n_slices=1000)\n",
    "\n",
    "#train_batches = dataset.train_dataset.batch(32)\n",
    "#for feature, target in train_batches.take(1):\n",
    "#    print(f\"Feature: {feature.shape}\")\n",
    "#    print(f\"Target: {target.shape}\")\n",
    "\n",
    "#print(len(dataset.train_dataset))\n",
    "#print(len(dataset.dev_dataset))\n",
    "#print(len(dataset.val_dataset))\n",
    "#This block of code iterates through the dataset and extracts the pt values of the labels and stores them in a list\n",
    "#data = [labels.numpy()[0] for features, labels in dataset.train_dataset.take(100000)]\n",
    "\n",
    "#plt.hist(data, bins=100, range=(50, 130), histtype='step', label='pt distribution', density=False)\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.title('pt distribution of the dataset')\n",
    "#plt.xlabel('pt')\n",
    "#plt.ylabel('Number of events')\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.make_slices(n_slices=100)\n",
    "slices = dataset.slices   \n",
    "\n",
    "lorentz_mask = tf.constant(dataset.get_lorentz_mask())  # Shape [35] bool values\n",
    "lorentz_indices = tf.squeeze(tf.where(lorentz_mask), axis=1) # [0 1 2 3 4 5 6 7 13 14 ...] \n",
    "n_vectors = tf.shape(lorentz_indices)[0] // 4 # number of 4-vectors\n",
    "lorentz_indices_4d = tf.reshape(lorentz_indices, (n_vectors, 4))  # [n_vectors, 4]\n",
    "\n",
    "@tf.function\n",
    "def augment_lorentz(data, target):\n",
    "    beta = tf.random.uniform(shape = (), minval=-0.98, maxval=0.98) # Shape ()\n",
    "    #tf.print(\"used beta: \", beta)\n",
    "    gamma = 1.0 / tf.sqrt(1.0 - beta**2) # Shape ()\n",
    "    #tf.print(\"used gamma: \", gamma)\n",
    "    \n",
    "    for i in range(n_vectors):\n",
    "        vec_indices = lorentz_indices_4d[i] # Shape (4,)\n",
    "        pt = data[vec_indices[0]] # scalar values\n",
    "        eta = data[vec_indices[1]]\n",
    "        #tf.print(\"eta: \", eta)\n",
    "        phi = data[vec_indices[2]]\n",
    "        E = data[vec_indices[3]]\n",
    "        #tf.print(\"E: \", E)\n",
    "        #print(E.shape)\n",
    "        \n",
    "        # Convert to Cartesian coordinates\n",
    "        #px = pt * tf.cos(phi)\n",
    "        #py = pt * tf.sin(phi)\n",
    "        pz = pt * tf.sinh(eta) # Shape ()\n",
    "        #tf.print(\"pz: \", pz)\n",
    "\n",
    "        E_prime = gamma * (E - beta * pz) # Shape ()\n",
    "        #tf.print(\"E_prime: \", E_prime)\n",
    "        pz_prime = gamma * (pz - beta * E) \n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        eta_prime = tf.asinh(pz_prime / (pt + epsilon)) # Shape ()\n",
    "        #tf.print(\"eta_prime: \", eta_prime)\n",
    "        update_indices = tf.reshape(vec_indices, [-1, 1])  # Shape (4, 1)\n",
    "        update_values = tf.stack([pt, eta_prime, phi, E_prime]) # Shape (4,)\n",
    "        # Update the tensor\n",
    "        data = tf.tensor_scatter_nd_update(\n",
    "            data,\n",
    "            indices=update_indices,\n",
    "            updates=update_values\n",
    "        )\n",
    "        #print(data.shape)\n",
    "        \n",
    "    return data, target\n",
    "\n",
    "n_events = 10000\n",
    "new_dataset = tf.data.Dataset.sample_from_datasets([s.repeat() for s in slices], weights=[1.]*len(slices))\n",
    "#orig_features, orig_masses = next(iter(new_dataset))\n",
    "#print(orig_features)\n",
    "#print(orig_masses)\n",
    "new_dataset = new_dataset.take(n_events)\n",
    "augmented_dataset = new_dataset.map(augment_lorentz)\n",
    "#new_features, new_masses = next(iter(new_dataset))\n",
    "#print(new_features)š\n",
    "#print(new_masses)\n",
    "#batch_dataset = augmented_dataset.batch(n_events)\n",
    "#print(f\"Number of events in batch_dataset: {len(augmented_dataset)}\")\n",
    "#features, masses = next(iter(batch_dataset))\n",
    "\n",
    "#plt.hist(masses, range=(70, 130), bins=50)\n",
    "#plt.show()\n",
    "\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True False False False False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False], shape=(35,), dtype=bool)\n",
      "tf.Tensor(\n",
      "[ 5.56329613e+01 -3.16706300e-01 -9.03481603e-01 -7.03347385e-01\n",
      "  5.47039528e+01 -1.49041474e-01  2.52283901e-01 -2.15698794e-01\n",
      "  1.67467535e-01  1.15576553e+00  1.16783524e+00  1.38526611e+02\n",
      "  1.10336914e+02  1.38102692e+02  1.78831533e-01  2.87319326e+00\n",
      "  1.64828949e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.71827660e+01 -1.32284049e-05 -6.81111366e-02\n",
      " -1.58981103e-02  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00], shape=(35,), dtype=float32)\n",
      "tf.Tensor(94.92591, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lorentz_mask = tf.constant(dataset.get_lorentz_mask()) \n",
    "print(lorentz_mask)\n",
    "\n",
    "features = tf.constant([\n",
    "    5.56329613e+01, -3.16469967e-01, -9.03481603e-01, 6.74349565e-07,\n",
    "    5.47039528e+01, -1.49002433e-01, 2.52283901e-01, 1.05658375e-01,\n",
    "    1.67467535e-01, 1.15576553e+00, 1.16783524e+00, 1.38526611e+02,\n",
    "    1.10336914e+02, 1.38102692e+02, 1.74350947e-01, 2.87319326e+00,\n",
    "    1.55200872e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
    "    0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
    "    0.00000000e+00, 4.71827660e+01, 0.00000000e+00, -6.81111366e-02,\n",
    "    -1.58858541e-02, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "    0.00000000e+00, 1.00000000e+00, 1.00000000e+00\n",
    "], dtype=tf.float32)\n",
    "\n",
    "label = tf.constant(94.92591, dtype=tf.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensors((features, label))\n",
    "dataset = dataset.map(augment_lorentz)\n",
    "\n",
    "new_features, new_label = next(iter(dataset))\n",
    "print(new_features)\n",
    "print(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.make_slices(n_slices=10)\n",
    "slices = dataset.slices \n",
    "phi_mask = tf.constant(dataset.get_phi_mask())\n",
    "\n",
    "@tf.function\n",
    "def augment_phi(data, target):\n",
    "    # generate random rotation angle\n",
    "    angle = tf.random.uniform(shape=(tf.shape(data)[0],), minval=-np.pi, maxval=np.pi)\n",
    "    #tf.print(\"angle: \", angle.shape)\n",
    "    #tf.print(\"data: \", data.shape)\n",
    "    # apply rotation\n",
    "    data  = tf.where(phi_mask, data + angle, data)\n",
    "    \n",
    "    # normalize angles between -pi and pi\n",
    "    data = tf.where(phi_mask, tf.math.atan2(tf.sin(data), tf.cos(data)), data)\n",
    "    \n",
    "    return data, target\n",
    "\n",
    "# sample from the slices\n",
    "n_events = 1000\n",
    "new_dataset = tf.data.Dataset.sample_from_datasets([s.repeat() for s in slices], weights=[1.]*len(slices))\n",
    "new_dataset = new_dataset.take(n_events)\n",
    "\n",
    "# apply augmentation\n",
    "new_dataset = new_dataset.map(augment_phi)\n",
    "#aug_dataset = new_dataset.batch(n_events)\n",
    "\n",
    "\n",
    "#print(f\"Number of events in batch_dataset: {len(batch_dataset)}\")\n",
    "#features, masses = next(iter(batch_dataset))\n",
    "#print(features.shape)\n",
    "#print(features)\n",
    "\n",
    "#plt.hist(masses, range=(70, 130), bins=50)\n",
    "#plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching datasets...\n",
      "Building model...\n",
      "Training model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 20:42:51.802874: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 151ms/step - loss: 2335.7473 - mean_squared_error: 2335.7437 - val_loss: 312.1779 - val_mean_squared_error: 312.4890\n",
      "Epoch 2/5\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 150ms/step - loss: 251.6451 - mean_squared_error: 251.6452 - val_loss: 255.4529 - val_mean_squared_error: 255.5452\n",
      "Epoch 3/5\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 147ms/step - loss: 221.5373 - mean_squared_error: 221.5372 - val_loss: 222.5968 - val_mean_squared_error: 222.6384\n",
      "Epoch 4/5\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 149ms/step - loss: 208.6559 - mean_squared_error: 208.6557 - val_loss: 204.9115 - val_mean_squared_error: 204.9481\n",
      "Epoch 5/5\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 152ms/step - loss: 201.6959 - mean_squared_error: 201.6958 - val_loss: 201.3717 - val_mean_squared_error: 201.4131\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RegressionModel.plot_history() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_model()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: RegressionModel.plot_history() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'batch_size': [3200, 6400],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'epochs': [5, 20, 30]\n",
    "}\n",
    "\n",
    "iterable = list(itertools.product(*param_grid.values()))\n",
    "for params in iterable:\n",
    "    model = RegressionModel(dataset=dataset, batch_size=params[0], initial_learning_rate=params[1], n_epochs=params[2])\n",
    "    model.prepare_dataset()\n",
    "    model.create_normalizer()\n",
    "    model.build_model()\n",
    "    model.train_model()\n",
    "    model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-17 21:09:41,674] A new study created in RDB with name: Higgs_analysis_mass\n",
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://0.0.0.0:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n",
      "2025-03-17 21:09:41.952477: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching datasets...\n",
      "Building model...\n",
      "Training model...\n",
      "Epoch 1/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 3ms/step - loss: 380.2455 - mean_squared_error: 380.2455 - val_loss: 239.6308 - val_mean_squared_error: 239.6303\n",
      "Epoch 2/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3ms/step - loss: 215.0253 - mean_squared_error: 215.0253 - val_loss: 226.2801 - val_mean_squared_error: 226.2794\n",
      "Epoch 3/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3ms/step - loss: 204.9460 - mean_squared_error: 204.9460 - val_loss: 194.8193 - val_mean_squared_error: 194.8188\n",
      "Epoch 4/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3ms/step - loss: 196.0404 - mean_squared_error: 196.0404 - val_loss: 180.9605 - val_mean_squared_error: 180.9600\n",
      "Epoch 5/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3ms/step - loss: 183.5660 - mean_squared_error: 183.5660 - val_loss: 184.2655 - val_mean_squared_error: 184.2651\n",
      "Epoch 6/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3ms/step - loss: 168.1545 - mean_squared_error: 168.1545 - val_loss: 202.0643 - val_mean_squared_error: 202.0639\n",
      "Epoch 7/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3ms/step - loss: 164.3124 - mean_squared_error: 164.3124 - val_loss: 160.5715 - val_mean_squared_error: 160.5711\n",
      "Epoch 8/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3ms/step - loss: 162.2302 - mean_squared_error: 162.2302 - val_loss: 177.4445 - val_mean_squared_error: 177.4441\n",
      "Epoch 9/25\n",
      "\u001b[1m30465/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 3ms/step - loss: 159.0238 - mean_squared_error: 159.0238 - val_loss: 161.2264 - val_mean_squared_error: 161.2260\n",
      "Epoch 10/25\n",
      "\u001b[1m30439/30465\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 158.3445 - mean_squared_error: 158.3445"
     ]
    }
   ],
   "source": [
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model = RegressionModel(\n",
    "        dataset,\n",
    "        n_layers             = trial.suggest_int('n_layers', 1, 5),\n",
    "        hidden_layer_size    = trial.suggest_int('hidden_layer_size', 32, 512),\n",
    "        initial_learning_rate= trial.suggest_float('initial_learning_rate', 1e-4, 1e-2, log=True),\n",
    "        n_epochs             = trial.suggest_int('n_epochs', 5, 30, step=5),\n",
    "        activation_function  = 'relu',\n",
    "        batch_size           = trial.suggest_int('batch_size', 32,128,step=32)\n",
    "    )\n",
    "\n",
    "    model.prepare_dataset()\n",
    "    model.create_normalizer()\n",
    "    model.build_model()\n",
    "    model.train_model()\n",
    "\n",
    "    val_loss, _ = model.model.evaluate(model.val_batch, verbose=0)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(storage=\"sqlite:///optuna.db\", study_name=\"Higgs_analysis_mass\", direction='minimize')\n",
    "\n",
    "def run_dashboard():\n",
    "    server = run_server(\"sqlite:///optuna.db\", host=\"0.0.0.0\", port=8080)\n",
    "    server.run()  # Run dashboard continuously in a separate thread\n",
    "\n",
    "dashboard_thread = threading.Thread(target=run_dashboard)\n",
    "dashboard_thread.daemon = True  # Ensures it stops when the script ends\n",
    "dashboard_thread.start()\n",
    "\n",
    "def run_optuna():\n",
    "    study.optimize(objective, n_trials=100, n_jobs=1)\n",
    "\n",
    "# Run Optuna in a separate thread so the dashboard can be accessed in real-time\n",
    "optuna_thread = threading.Thread(target=run_optuna)\n",
    "optuna_thread.start()\n",
    "\n",
    "optuna_thread.join()  # Wait for Optuna to finish before printing results\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf gpu",
   "language": "python",
   "name": "tf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
