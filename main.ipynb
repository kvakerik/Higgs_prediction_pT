{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:19:48.809096: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-23 16:19:48.819015: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742743188.831310  823389 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742743188.835077  823389 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-23 16:19:48.849028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from ModelClass import RegressionModel\n",
    "from DatasetClass import Dataset, DatasetMass\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from src.helpers import make_filter_slice\n",
    "import tensorflow as tf\n",
    "import optuna \n",
    "from optuna_dashboard import run_server\n",
    "import threading\n",
    "#import sys\n",
    "#import logging \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "erik_data = \"/scratch/ucjf-atlas/htautau/SM_Htautau_R22/V02_skim_mva_01/*/*/*/*/*H125*.root\"\n",
    "patrik_data = \"/scratch/ucjf-atlas/htautau/SM_Htautau_R22/V02_skim_mva_01/*/*/*/*/*Ztt*.root\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742743197.531564  823389 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43493 MB memory:  -> device: 0, name: NVIDIA L40S, pci bus id: 0000:41:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7799005\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetMass(file_paths=patrik_data, file_name = \"data\")\n",
    "dataset.load_data()\n",
    "print(dataset.train_events)\n",
    "    \n",
    "#dataset.augment_data_phi(n_slices=1000)\n",
    "\n",
    "#train_batches = dataset.train_dataset.batch(32)\n",
    "#for feature, target in train_batches.take(1):\n",
    "#    print(f\"Feature: {feature.shape}\")\n",
    "#    print(f\"Target: {target.shape}\")\n",
    "\n",
    "#print(len(dataset.train_dataset))\n",
    "#print(len(dataset.dev_dataset))\n",
    "#print(len(dataset.val_dataset))\n",
    "#This block of code iterates through the dataset and extracts the pt values of the labels and stores them in a list\n",
    "#data = [labels.numpy()[0] for features, labels in dataset.train_dataset.take(100000)]\n",
    "\n",
    "#plt.hist(data, bins=100, range=(50, 130), histtype='step', label='pt distribution', density=False)\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.title('pt distribution of the dataset')\n",
    "#plt.xlabel('pt')\n",
    "#plt.ylabel('Number of events')\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.make_slices(n_slices=100)\n",
    "slices = dataset.slices   \n",
    "\n",
    "lorentz_mask = tf.constant(dataset.get_lorentz_mask())  # Shape [35] bool values\n",
    "lorentz_indices = tf.squeeze(tf.where(lorentz_mask), axis=1) # [0 1 2 3 4 5 6 7 13 14 ...] \n",
    "n_vectors = tf.shape(lorentz_indices)[0] // 4 # number of 4-vectors\n",
    "lorentz_indices_4d = tf.reshape(lorentz_indices, (n_vectors, 4))  # [n_vectors, 4]\n",
    "\n",
    "@tf.function\n",
    "def augment_lorentz(data, target):\n",
    "    beta = tf.random.uniform(shape = (), minval=-0.98, maxval=0.98) # Shape ()\n",
    "    #tf.print(\"used beta: \", beta)\n",
    "    gamma = 1.0 / tf.sqrt(1.0 - beta**2) # Shape ()\n",
    "    #tf.print(\"used gamma: \", gamma)\n",
    "    \n",
    "    for i in range(n_vectors):\n",
    "        vec_indices = lorentz_indices_4d[i] # Shape (4,)\n",
    "        pt = data[vec_indices[0]] # scalar values\n",
    "        eta = data[vec_indices[1]]\n",
    "        #tf.print(\"eta: \", eta)\n",
    "        phi = data[vec_indices[2]]\n",
    "        E = data[vec_indices[3]]\n",
    "        #tf.print(\"E: \", E)\n",
    "        #print(E.shape)\n",
    "        \n",
    "        # Convert to Cartesian coordinates\n",
    "        #px = pt * tf.cos(phi)\n",
    "        #py = pt * tf.sin(phi)\n",
    "        pz = pt * tf.sinh(eta) # Shape ()\n",
    "        #tf.print(\"pz: \", pz)\n",
    "\n",
    "        E_prime = gamma * (E - beta * pz) # Shape ()\n",
    "        #tf.print(\"E_prime: \", E_prime)\n",
    "        pz_prime = gamma * (pz - beta * E) \n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        eta_prime = tf.asinh(pz_prime / (pt + epsilon)) # Shape ()\n",
    "        #tf.print(\"eta_prime: \", eta_prime)\n",
    "        update_indices = tf.reshape(vec_indices, [-1, 1])  # Shape (4, 1)\n",
    "        update_values = tf.stack([pt, eta_prime, phi, E_prime]) # Shape (4,)\n",
    "        # Update the tensor\n",
    "        data = tf.tensor_scatter_nd_update(\n",
    "            data,\n",
    "            indices=update_indices,\n",
    "            updates=update_values\n",
    "        )\n",
    "        #print(data.shape)\n",
    "        \n",
    "    return data, target\n",
    "\n",
    "n_events = 10000\n",
    "new_dataset = tf.data.Dataset.sample_from_datasets([s.repeat() for s in slices], weights=[1.]*len(slices))\n",
    "#orig_features, orig_masses = next(iter(new_dataset))\n",
    "#print(orig_features)\n",
    "#print(orig_masses)\n",
    "new_dataset = new_dataset.take(n_events)\n",
    "augmented_dataset = new_dataset.map(augment_lorentz)\n",
    "#new_features, new_masses = next(iter(new_dataset))\n",
    "#print(new_features)Å¡\n",
    "#print(new_masses)\n",
    "#batch_dataset = augmented_dataset.batch(n_events)\n",
    "#print(f\"Number of events in batch_dataset: {len(augmented_dataset)}\")\n",
    "#features, masses = next(iter(batch_dataset))\n",
    "\n",
    "#plt.hist(masses, range=(70, 130), bins=50)\n",
    "#plt.show()\n",
    "\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True False False False False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False], shape=(35,), dtype=bool)\n",
      "tf.Tensor(\n",
      "[ 5.56329613e+01 -3.16706300e-01 -9.03481603e-01 -7.03347385e-01\n",
      "  5.47039528e+01 -1.49041474e-01  2.52283901e-01 -2.15698794e-01\n",
      "  1.67467535e-01  1.15576553e+00  1.16783524e+00  1.38526611e+02\n",
      "  1.10336914e+02  1.38102692e+02  1.78831533e-01  2.87319326e+00\n",
      "  1.64828949e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.71827660e+01 -1.32284049e-05 -6.81111366e-02\n",
      " -1.58981103e-02  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  1.00000000e+00], shape=(35,), dtype=float32)\n",
      "tf.Tensor(94.92591, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lorentz_mask = tf.constant(dataset.get_lorentz_mask()) \n",
    "print(lorentz_mask)\n",
    "\n",
    "features = tf.constant([\n",
    "    5.56329613e+01, -3.16469967e-01, -9.03481603e-01, 6.74349565e-07,\n",
    "    5.47039528e+01, -1.49002433e-01, 2.52283901e-01, 1.05658375e-01,\n",
    "    1.67467535e-01, 1.15576553e+00, 1.16783524e+00, 1.38526611e+02,\n",
    "    1.10336914e+02, 1.38102692e+02, 1.74350947e-01, 2.87319326e+00,\n",
    "    1.55200872e+01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
    "    0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
    "    0.00000000e+00, 4.71827660e+01, 0.00000000e+00, -6.81111366e-02,\n",
    "    -1.58858541e-02, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
    "    0.00000000e+00, 1.00000000e+00, 1.00000000e+00\n",
    "], dtype=tf.float32)\n",
    "\n",
    "label = tf.constant(94.92591, dtype=tf.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensors((features, label))\n",
    "dataset = dataset.map(augment_lorentz)\n",
    "\n",
    "new_features, new_label = next(iter(dataset))\n",
    "print(new_features)\n",
    "print(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.make_slices(n_slices=10)\n",
    "slices = dataset.slices \n",
    "phi_mask = tf.constant(dataset.get_phi_mask())\n",
    "\n",
    "@tf.function\n",
    "def augment_phi(data, target):\n",
    "    # generate random rotation angle\n",
    "    angle = tf.random.uniform(shape=(tf.shape(data)[0],), minval=-np.pi, maxval=np.pi)\n",
    "    #tf.print(\"angle: \", angle.shape)\n",
    "    #tf.print(\"data: \", data.shape)\n",
    "    # apply rotation\n",
    "    data  = tf.where(phi_mask, data + angle, data)\n",
    "    \n",
    "    # normalize angles between -pi and pi\n",
    "    data = tf.where(phi_mask, tf.math.atan2(tf.sin(data), tf.cos(data)), data)\n",
    "    \n",
    "    return data, target\n",
    "\n",
    "# sample from the slices\n",
    "n_events = 1000\n",
    "new_dataset = tf.data.Dataset.sample_from_datasets([s.repeat() for s in slices], weights=[1.]*len(slices))\n",
    "new_dataset = new_dataset.take(n_events)\n",
    "\n",
    "# apply augmentation\n",
    "new_dataset = new_dataset.map(augment_phi)\n",
    "#aug_dataset = new_dataset.batch(n_events)\n",
    "\n",
    "\n",
    "#print(f\"Number of events in batch_dataset: {len(batch_dataset)}\")\n",
    "#features, masses = next(iter(batch_dataset))\n",
    "#print(features.shape)\n",
    "#print(features)\n",
    "\n",
    "#plt.hist(masses, range=(70, 130), bins=50)\n",
    "#plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching datasets...\n",
      "Building model...\n",
      "Training model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 20:42:51.802874: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 151ms/step - loss: 2335.7473 - mean_squared_error: 2335.7437 - val_loss: 312.1779 - val_mean_squared_error: 312.4890\n",
      "Epoch 2/5\n",
      "\u001b[1m305/305\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 150ms/step - loss: 251.6451 - mean_squared_error: 251.6452 - val_loss: 255.4529 - val_mean_squared_error: 255.5452\n",
      "Epoch 3/5\n",
      "\u001b[1m305/305\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 147ms/step - loss: 221.5373 - mean_squared_error: 221.5372 - val_loss: 222.5968 - val_mean_squared_error: 222.6384\n",
      "Epoch 4/5\n",
      "\u001b[1m305/305\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 149ms/step - loss: 208.6559 - mean_squared_error: 208.6557 - val_loss: 204.9115 - val_mean_squared_error: 204.9481\n",
      "Epoch 5/5\n",
      "\u001b[1m305/305\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 152ms/step - loss: 201.6959 - mean_squared_error: 201.6958 - val_loss: 201.3717 - val_mean_squared_error: 201.4131\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RegressionModel.plot_history() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_model()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: RegressionModel.plot_history() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'batch_size': [3200, 6400],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'epochs': [5, 20, 30]\n",
    "}\n",
    "\n",
    "iterable = list(itertools.product(*param_grid.values()))\n",
    "for params in iterable:\n",
    "    model = RegressionModel(dataset=dataset, batch_size=params[0], initial_learning_rate=params[1], n_epochs=params[2])\n",
    "    model.prepare_dataset()\n",
    "    model.create_normalizer()\n",
    "    model.build_model()\n",
    "    model.train_model()\n",
    "    model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 16:20:34,767] Using an existing study with name 'Higgs_analysis_mass_2' instead of creating a new one.\n",
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://0.0.0.0:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:20:35.267286: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Training model...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742743236.809564  823505 service.cc:148] XLA service 0x7fa6840076c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1742743236.809601  823505 service.cc:156]   StreamExecutor device (0): NVIDIA L40S, Compute Capability 8.9\n",
      "2025-03-23 16:20:36.859300: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1742743237.056711  823505 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   6/2177\u001b[0m \u001b[37mââââââââââââââââââââ\u001b[0m \u001b[1m38s\u001b[0m 18ms/step - loss: 13788.5908 - mean_absolute_percentage_error: 97.5609 - mean_squared_error: 13788.5908  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742743238.392931  823505 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1408.0747 - mean_absolute_percentage_error: 21.2938 - mean_squared_error: 1408.0745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:22:09.523219: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-03-23 16:22:09.530842: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-03-23 16:22:09.568427: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 644 bytes spill stores, 668 bytes spill loads\n",
      "\n",
      "2025-03-23 16:22:09.599735: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 324 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2025-03-23 16:22:09.608085: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2025-03-23 16:22:20.912335: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 380 bytes spill stores, 380 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 47ms/step - loss: 1407.6812 - mean_absolute_percentage_error: 21.2901 - mean_squared_error: 1407.6808 - val_loss: 1280.6429 - val_mean_absolute_percentage_error: 27.9558 - val_mean_squared_error: 1276.4458\n",
      "Epoch 2/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 47ms/step - loss: 274.8246 - mean_absolute_percentage_error: 9.4243 - mean_squared_error: 274.8245 - val_loss: 778.8183 - val_mean_absolute_percentage_error: 16.5713 - val_mean_squared_error: 776.7018\n",
      "Epoch 3/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 47ms/step - loss: 251.4580 - mean_absolute_percentage_error: 8.5762 - mean_squared_error: 251.4580 - val_loss: 591.9178 - val_mean_absolute_percentage_error: 10.3805 - val_mean_squared_error: 591.2197\n",
      "Epoch 4/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 46ms/step - loss: 242.0949 - mean_absolute_percentage_error: 8.0299 - mean_squared_error: 242.0951 - val_loss: 324.8477 - val_mean_absolute_percentage_error: 5.9751 - val_mean_squared_error: 324.3288\n",
      "Epoch 5/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 44ms/step - loss: 232.5398 - mean_absolute_percentage_error: 7.6163 - mean_squared_error: 232.5399 - val_loss: 437.0224 - val_mean_absolute_percentage_error: 5.7620 - val_mean_squared_error: 435.7019\n",
      "Epoch 6/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 45ms/step - loss: 221.7677 - mean_absolute_percentage_error: 7.3682 - mean_squared_error: 221.7677 - val_loss: 260.4118 - val_mean_absolute_percentage_error: 6.4257 - val_mean_squared_error: 259.8386\n",
      "Epoch 7/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 44ms/step - loss: 215.8575 - mean_absolute_percentage_error: 7.1205 - mean_squared_error: 215.8575 - val_loss: 327.5907 - val_mean_absolute_percentage_error: 6.2898 - val_mean_squared_error: 326.6030\n",
      "Epoch 8/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 44ms/step - loss: 205.2907 - mean_absolute_percentage_error: 6.9861 - mean_squared_error: 205.2907 - val_loss: 283.0088 - val_mean_absolute_percentage_error: 7.6916 - val_mean_squared_error: 282.5247\n",
      "Epoch 9/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 45ms/step - loss: 201.5104 - mean_absolute_percentage_error: 6.8723 - mean_squared_error: 201.5105 - val_loss: 288.1566 - val_mean_absolute_percentage_error: 5.9345 - val_mean_squared_error: 287.4786\n",
      "Epoch 10/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 44ms/step - loss: 192.6249 - mean_absolute_percentage_error: 6.7607 - mean_squared_error: 192.6248 - val_loss: 309.7852 - val_mean_absolute_percentage_error: 7.2494 - val_mean_squared_error: 309.5112\n",
      "Epoch 11/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 46ms/step - loss: 185.5713 - mean_absolute_percentage_error: 6.6591 - mean_squared_error: 185.5712 - val_loss: 280.6339 - val_mean_absolute_percentage_error: 7.6305 - val_mean_squared_error: 280.8672\n",
      "Epoch 12/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 45ms/step - loss: 180.0854 - mean_absolute_percentage_error: 6.5549 - mean_squared_error: 180.0855 - val_loss: 280.8667 - val_mean_absolute_percentage_error: 7.8828 - val_mean_squared_error: 281.2033\n",
      "Epoch 13/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 45ms/step - loss: 175.9425 - mean_absolute_percentage_error: 6.4922 - mean_squared_error: 175.9425 - val_loss: 282.9878 - val_mean_absolute_percentage_error: 8.1843 - val_mean_squared_error: 283.6408\n",
      "Epoch 14/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 45ms/step - loss: 174.3506 - mean_absolute_percentage_error: 6.4537 - mean_squared_error: 174.3505 - val_loss: 314.7535 - val_mean_absolute_percentage_error: 8.8367 - val_mean_squared_error: 315.9789\n",
      "Epoch 15/15\n",
      "\u001b[1m2177/2177\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 44ms/step - loss: 171.9882 - mean_absolute_percentage_error: 6.4317 - mean_squared_error: 171.9882 - val_loss: 311.4539 - val_mean_absolute_percentage_error: 8.7419 - val_mean_squared_error: 312.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 16:45:24,648] Trial 8 finished with value: 311.453857421875 and parameters: {'n_layers': 6, 'hidden_layer_size': 1024, 'initial_learning_rate': 0.007994985434454184, 'n_epochs': 15, 'batch_size': 3584, 'dropout_rate': 0.20719556411060586, 'weight_decay': 0.0005187752223469123}. Best is trial 0 with value: 139.28695678710938.\n",
      "2025-03-23 16:45:24.829392: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching datasets...\n",
      "Building model...\n",
      "Training model...\n",
      "Epoch 1/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 608.4227 - mean_absolute_percentage_error: 13.5498 - mean_squared_error: 608.4227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 16:46:59.997643: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 8 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-03-23 16:47:00.063412: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 92 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2025-03-23 16:47:00.141932: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 460 bytes spill stores, 460 bytes spill loads\n",
      "\n",
      "2025-03-23 16:47:00.166776: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 788 bytes spill stores, 788 bytes spill loads\n",
      "\n",
      "2025-03-23 16:47:00.303412: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_53', 300 bytes spill stores, 300 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 27ms/step - loss: 608.3461 - mean_absolute_percentage_error: 13.5488 - mean_squared_error: 608.3460 - val_loss: 283.8860 - val_mean_absolute_percentage_error: 8.2756 - val_mean_squared_error: 283.4986\n",
      "Epoch 2/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 26ms/step - loss: 218.6232 - mean_absolute_percentage_error: 7.9146 - mean_squared_error: 218.6232 - val_loss: 220.6134 - val_mean_absolute_percentage_error: 5.8661 - val_mean_squared_error: 220.4190\n",
      "Epoch 3/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 26ms/step - loss: 208.1542 - mean_absolute_percentage_error: 7.6094 - mean_squared_error: 208.1542 - val_loss: 201.9547 - val_mean_absolute_percentage_error: 5.2379 - val_mean_squared_error: 201.7931\n",
      "Epoch 4/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 26ms/step - loss: 200.1200 - mean_absolute_percentage_error: 7.3411 - mean_squared_error: 200.1200 - val_loss: 192.6185 - val_mean_absolute_percentage_error: 5.3341 - val_mean_squared_error: 192.6055\n",
      "Epoch 5/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 25ms/step - loss: 194.5458 - mean_absolute_percentage_error: 7.1851 - mean_squared_error: 194.5458 - val_loss: 162.0352 - val_mean_absolute_percentage_error: 5.3184 - val_mean_squared_error: 162.2370\n",
      "Epoch 6/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 26ms/step - loss: 190.3524 - mean_absolute_percentage_error: 7.0644 - mean_squared_error: 190.3524 - val_loss: 169.5264 - val_mean_absolute_percentage_error: 5.4817 - val_mean_squared_error: 169.8612\n",
      "Epoch 7/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 25ms/step - loss: 187.5204 - mean_absolute_percentage_error: 6.9714 - mean_squared_error: 187.5204 - val_loss: 157.0620 - val_mean_absolute_percentage_error: 5.9881 - val_mean_squared_error: 157.4882\n",
      "Epoch 8/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 25ms/step - loss: 182.4200 - mean_absolute_percentage_error: 6.8596 - mean_squared_error: 182.4200 - val_loss: 176.0170 - val_mean_absolute_percentage_error: 6.3787 - val_mean_squared_error: 176.5522\n",
      "Epoch 9/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 25ms/step - loss: 180.4600 - mean_absolute_percentage_error: 6.8006 - mean_squared_error: 180.4600 - val_loss: 173.0675 - val_mean_absolute_percentage_error: 7.5575 - val_mean_squared_error: 174.0177\n",
      "Epoch 10/20\n",
      "\u001b[1m3809/3809\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 25ms/step - loss: 177.9178 - mean_absolute_percentage_error: 6.7150 - mean_squared_error: 177.9178 - val_loss: 183.7418 - val_mean_absolute_percentage_error: 7.6423 - val_mean_squared_error: 184.6240\n",
      "Epoch 11/20\n",
      "\u001b[1m3806/3809\u001b[0m \u001b[32mâââââââââââââââââââ\u001b[0m\u001b[37mâ\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 174.6883 - mean_absolute_percentage_error: 6.6576 - mean_squared_error: 174.6883"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    model = RegressionModel(\n",
    "                dataset,\n",
    "                n_layers             = trial.suggest_int('n_layers', 4, 6),\n",
    "                hidden_layer_size    = trial.suggest_int('hidden_layer_size', 1024, 2048, step=256),\n",
    "                initial_learning_rate= trial.suggest_float('initial_learning_rate', 1e-3, 1e-2, log=True),\n",
    "                n_epochs             = trial.suggest_int('n_epochs', 10, 20, step=5),\n",
    "                activation_function  = 'relu',\n",
    "                batch_size           = trial.suggest_int('batch_size', 1024, 6144,step=512),\n",
    "                dropout_rate         = trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "                weight_decay         = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "\n",
    "            )\n",
    "\n",
    "    model.prepare_dataset()\n",
    "    model.create_normalizer()\n",
    "    model.build_model()\n",
    "    model.train_model()\n",
    "\n",
    "    # Handle based on the number of metrics\n",
    "    evaluation_results = model.model.evaluate(model.dev_batch, verbose=0)\n",
    "    if isinstance(evaluation_results, list):\n",
    "        val_loss = evaluation_results[0]\n",
    "    else:\n",
    "        val_loss = evaluation_results  # If only one value is returned, it's the loss\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(storage=\"sqlite:///optuna.db\", study_name=\"Higgs_analysis_mass_2\", direction='minimize', load_if_exists=True)\n",
    "\n",
    "def run_dashboard():\n",
    "    server = run_server(\"sqlite:///optuna.db\", host=\"0.0.0.0\", port=8080)\n",
    "    server.run()  # Run dashboard continuously in a separate thread\n",
    "\n",
    "dashboard_thread = threading.Thread(target=run_dashboard)\n",
    "dashboard_thread.daemon = True  # Ensures it stops when the script ends\n",
    "dashboard_thread.start()\n",
    "\n",
    "def run_optuna():\n",
    "    study.optimize(objective, n_trials=100, n_jobs=1)\n",
    "\n",
    "# Run Optuna in a separate thread so the dashboard can be accessed in real-time\n",
    "optuna_thread = threading.Thread(target=run_optuna)\n",
    "optuna_thread.start()\n",
    "\n",
    "optuna_thread.join()  # Wait for Optuna to finish before printing results\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htt_env",
   "language": "python",
   "name": "htt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
