{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "# limit the memory ussage of GPU\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset parameters\n",
    "N_VAL = 20000 #177781\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "\n",
    "# load data and convert them to awkward arrays using uproot\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "file_sig = \"/scratch/ucjf-atlas/njsf164/data_4top_root/user.nhidic.412043.aMcAtNloPythia8EvtGen.DAOD_PHYS.e7101_a907_r14860_p5855.4thad26_240130_v06.3_output_root.nominal.root\"\n",
    "file_bkg = \"/scratch/ucjf-atlas/njsf164/data_4top_root/user.nhidic.410471.PhPy8EG.DAOD_PHYS.e6337_s3681_r13145_p5855.4thad26_240123_v06.2_output_root.nominal.root\"\n",
    "\n",
    "# load the tree\n",
    "tree_sig = uproot.open(file_sig)['nominal']\n",
    "tree_bkg = uproot.open(file_bkg)['nominal']\n",
    "\n",
    "# load the branches\n",
    "branches = ['jet_pt', 'jet_eta', 'jet_phi', 'jet_e', 'jet_tagWeightBin_DL1dv01_Continuous']\n",
    "data_sig = tree_sig.arrays(branches, library='ak', entry_stop=177781)\n",
    "data_bkg = tree_bkg.arrays(branches, library='ak', entry_stop=177781)\n",
    "\n",
    "\n",
    "## convert awkward arrays to tensorflow tensors\n",
    "tensors = []\n",
    "for i,arrays in enumerate([data_sig, data_bkg]):\n",
    "    tensors.append([])\n",
    "    for var in branches:\n",
    "        array = arrays[var]\n",
    "        row_lengths = ak.num(array, axis=1).to_numpy()\n",
    "        tensor = tf.RaggedTensor.from_row_lengths(ak.flatten(array, axis=None).to_numpy(), row_lengths=row_lengths, validate=False)\n",
    "        tensor = tf.cast(tensor, tf.float32)\n",
    "        tensors[i].append(tensor)\n",
    "\n",
    "## calculate mean and std. dev. of the tensors\n",
    "mean = [tf.reduce_mean(tf.concat([tensor_sig,tensor_bkg], axis=0)) for tensor_sig,tensor_bkg in zip(tensors[0], tensors[1])]\n",
    "std_dev = [tf.math.reduce_std(tf.concat([tensor_sig,tensor_bkg], axis=0)) for tensor_sig,tensor_bkg in zip(tensors[0], tensors[1])]\n",
    "\n",
    "for var,m,v in zip(branches,mean, std_dev):\n",
    "    print(f'{var} - mean: {m.numpy()}, std_dev: {v.numpy()}')\n",
    "\n",
    "\n",
    "# normalize the tensors. Subtract mean and divide by variance\n",
    "for i in range(len(tensors)):\n",
    "    for j in range(len(tensors[i])):\n",
    "        tensors[i][j] = (tensors[i][j] - mean[j]) / std_dev[j]\n",
    "\n",
    "## stack tensors of individual branches to create a single 3D tensor\n",
    "for i in range(len(tensors)):\n",
    "    tensors[i] = tf.stack(tensors[i], axis=-1)\n",
    "\n",
    "\n",
    "print(tensors[0].shape)\n",
    "print(tensors[1].shape)\n",
    "\n",
    "# get the dataset size\n",
    "n_total = 2*min(tensors[0].shape[0], tensors[1].shape[0])\n",
    "\n",
    "## create signal and background datasets\n",
    "sig_dataset = tf.data.Dataset.from_tensor_slices(tensors[0])\n",
    "bkg_dataset = tf.data.Dataset.from_tensor_slices(tensors[1])\n",
    "\n",
    "## add labels to the datasets\n",
    "sig_dataset_labels = sig_dataset.map(lambda x: (x, 1))\n",
    "bkg_dataset_labels = bkg_dataset.map(lambda x: (x, 0))\n",
    "\n",
    "# combine datasets into a single dataset\n",
    "dataset = tf.data.Dataset.sample_from_datasets( [sig_dataset_labels, bkg_dataset_labels], weights=[0.5, 0.5], stop_on_empty_dataset=True)\n",
    "\n",
    "# split into training and validation datasets\n",
    "val_dataset = dataset.take(N_VAL)\n",
    "train_dataset = dataset.skip(N_VAL)\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_dataset = train_dataset.shuffle(10000).ragged_batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.ragged_batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset_size = n_total - N_VAL\n",
    "print(f'Training dataset size: {train_dataset_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10             # number of training epochs\n",
    "\n",
    "N_BLOCKS = 1           # number of blocks of bi-directional RNN layers\n",
    "HIDDEN_SIZE = 64        # number of hidden units in the RNN layers\n",
    "DROPOUT = 0.           # dropout rate\n",
    "\n",
    "## create the input layer\n",
    "inputShape = train_dataset.element_spec[0].shape  # this returns the input tensor shape including the batch dimension\n",
    "print(\"Input shape: \", inputShape[1:])\n",
    "input = tf.keras.layers.Input(shape=inputShape[1:], ragged=True) # here we provide shape without the batch dimension\n",
    "layer = input\n",
    "\n",
    "# create the mask\n",
    "mask = tf.sequence_mask(row_lengths)\n",
    "\n",
    "# we have to expand the last dimension of the input tensor to HIDDEN_SIZE\n",
    "layer = tf.keras.layers.Dense(HIDDEN_SIZE)(layer)\n",
    "\n",
    "## create the bi-directional RNN layers (using gated recurrent units, GRU)\n",
    "for i in range(N_BLOCKS):\n",
    "    layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(HIDDEN_SIZE, return_sequences=True, dropout=DROPOUT))(layer)\n",
    "    # layer = tf.keras.layers.LayerNormalization()(layer)\n",
    "\n",
    "## global max pooling\n",
    "layer = tf.keras.layers.GlobalMaxPooling1D()(layer)\n",
    "\n",
    "## final dense layer with sigmoid activation\n",
    "layer = tf.keras.layers.Dense(1, activation='sigmoid')(layer)\n",
    "\n",
    "\n",
    "## create the model\n",
    "model = tf.keras.Model(inputs=input, outputs=layer)\n",
    "\n",
    "# cosine learning rate decay\n",
    "initial_learning_rate = 0.001\n",
    "learning_rate = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate = initial_learning_rate, \n",
    "    decay_steps=EPOCHS * train_dataset_size,\n",
    "    alpha=0.01 * initial_learning_rate)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate = learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset)\n",
    "\n",
    "# evaluate the model\n",
    "metrices_train = model.evaluate(train_dataset)\n",
    "metrices_val = model.evaluate(val_dataset)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Overfitting check:\")\n",
    "print(\"\")\n",
    "for name, value_train, value_val in zip(model.metrics_names, metrices_train, metrices_val):\n",
    "    print(name, \"train:\", value_train, \"val:\", value_val)\n",
    "\n",
    "# draw training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='training')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['binary_accuracy'], label='training')\n",
    "plt.plot(history.history['val_binary_accuracy'], label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# The following function just picks data tensor from the (data, label) dataset\n",
    "@ tf.function\n",
    "def pick_only_data(data, label):\n",
    "    return data\n",
    "\n",
    "# draw signal and background separately\n",
    "output_sig_val = model.predict(val_dataset.unbatch().filter(lambda x, label: label == 1).map(pick_only_data).ragged_batch(1024))\n",
    "output_bkg_val = model.predict(val_dataset.unbatch().filter(lambda x, label: label == 0).map(pick_only_data).ragged_batch(1024))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(output_sig_val, histtype='step', bins=100, alpha=0.5, label='signal (val)', range=(0, 1))\n",
    "plt.hist(output_bkg_val, histtype='step', bins=100, alpha=0.5, label='background (val)', range=(0, 1))\n",
    "\n",
    "plt.xlabel('Output probability')\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf gpu",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
